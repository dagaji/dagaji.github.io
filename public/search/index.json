[{"content":"Project Description Last time, we implemented a web scraper to extract game reviews and store them in a MongoDB collection. Although the last project was valuable on its own, it would be nice to extend it so that we can also explore the data we have collected. One solution is to create a website that allows us to make queries to the MongoDB collection and display the results.\nThere are a number of full-featured web frameworks available in Python (Django, Pyramid, Web2py, etc ), but as our website will be a simple one, a lightweight framework like Flask will suffice. As for the interaction with MongoDB, we are already familiar with pymongo, as we used it before to store the scraped data into the collection, so we will use it again. Finally, in the front-end, we will make use of AJAX to implement some sort of pagination on the results without having to reload the page.\nBefore we get to work, I will show you a screenshot of the final webpage for you to get an idea of the intended result :\n Screenshot \nAs you can see our website allows to make the following queries:\n Filter reviews by genre, platform, reviewer and posted date. Search reviews by game name. Sort the displayed results either by score or posted date.  MongoDB queries There will be two types of queries in our application: 1) search by name queries; and 2) filter queries. The latter ones have two types of conditions: equality conditions (using the $and operator if more than one condition on the same field) and range conditions (using $gt and/or $lt). Also in the latter case, it is possible to sort the results either by posted date or score of the review.\ndef _process_result(result): result[\u0026#34;genre\u0026#34;] = \u0026#34;, \u0026#34;.join(result[\u0026#34;genre\u0026#34;]) result[\u0026#34;platforms\u0026#34;] = \u0026#34;, \u0026#34;.join(result[\u0026#34;platforms\u0026#34;]) result[\u0026#34;release_date\u0026#34;] = str(result[\u0026#34;release_date\u0026#34;]).split(\u0026#39; \u0026#39;)[0] result[\u0026#34;score\u0026#34;] = int(result[\u0026#34;score\u0026#34;]) return result def list_reviews(collection, filter_dict, sort_by=\u0026#34;release_date\u0026#34;, offset=0, page_size=1): cursor = collection.find(filter_dict).sort(sort_by, pymongo.DESCENDING) if offset \u0026gt; 0: cursor = cursor.skip(offset) cursor = cursor.limit(page_size+1) results = [_process_result(result) for result in cursor] has_next = len(results) \u0026gt; page_size if has_next: return results[:-1], has_next return results, has_next def search_by_name(collection, name, max_number_results=5): query = {\u0026#34;game_name\u0026#34;:{\u0026#34;$regex\u0026#34;: name, \u0026#34;$options\u0026#34;: \u0026#39;i\u0026#39;}} cursor = collection.find(query).limit(max_number_results) return [_process_result(result) for result in cursor] def build_query(eq_filters, range_filters): filter_dict = {} and_cond = [] for field, eq_conds in eq_filters.items(): if len(eq_conds) == 1: filter_dict[field] = eq_conds[0] else: and_cond += [{field:value} for value in eq_conds] if and_cond: filter_dict[\u0026#34;$and\u0026#34;] = and_cond for range_field, (inf_limit, sup_limit) in range_filters.items(): _range_filter = {range_field: {}} if inf_limit: _range_filter[range_field][\u0026#34;$gt\u0026#34;] = inf_limit if sup_limit: _range_filter[range_field][\u0026#34;$lt\u0026#34;] = sup_limit if _range_filter[range_field]: filter_dict.update(_range_filter) return filter_dict Note that we request page_size+1 elements in list_reviews() so that we can check if there are more than PAGE_SIZE matches. This will be useful later on, when we implement the pagination of results.\nFlask Routing and Jinja Templates There will be only two URLs (or routes) in our application: 1) “/index” (or just “/”) used by the end user to access the webpage in the browser; and 2) “/list” used by the JavaScript code to request results by means of AJAX.\nThe view function associated with the first route is really simple as it just renders a Jinja Template. This template is what actually does all the heavy lifting here, so we will focus on it.\n{% macro display_options(opts_name, opts_type, items, type) -%} \u0026lt;div class=\u0026#34;dropdown pull-left\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn btn-default dropdown-toggle\u0026#34; type=\u0026#34;button\u0026#34; id=\u0026#34;dropdownMenu1\u0026#34; data-toggle=\u0026#34;dropdown\u0026#34; aria-haspopup=\u0026#34;true\u0026#34; aria-expanded=\u0026#34;true\u0026#34; name=\u0026#34;{{opts_type}}\u0026#34;\u0026gt; {{opts_name}} \u0026lt;span class=\u0026#34;caret\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;ul class=\u0026#34;dropdown-menu checkbox-menu allow-focus\u0026#34; aria-labelledby=\u0026#34;dropdownMenu1\u0026#34;\u0026gt; {% for value in items %} \u0026lt;li\u0026gt; \u0026lt;label\u0026gt; \u0026lt;input type=\u0026#34;{{type}}\u0026#34; autocomplete=\u0026#34;off\u0026#34; name=\u0026#34;{{value}}\u0026#34;\u0026gt; {{value}} \u0026lt;/label\u0026gt; \u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; {%- endmacro %} \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Game Reviews\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;css/bootstrap.min.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;css/styles.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Oxygen:400,300,700\u0026#39; rel=\u0026#39;stylesheet\u0026#39; type=\u0026#39;text/css\u0026#39;\u0026gt; \u0026lt;link href=\u0026#39;https://fonts.googleapis.com/css?family=Lora\u0026#39; rel=\u0026#39;stylesheet\u0026#39; type=\u0026#39;text/css\u0026#39;\u0026gt; \u0026lt;script\u0026gt; const request_params = {{ request.args| tojson | safe}}; const request_url = \u0026#34;{{ url_for(\u0026#39;list_reviews\u0026#39;) }}\u0026#34;; \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ajax-utils.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/jquery-2.1.4.min.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/bootstrap.min.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/script.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;nav class=\u0026#34;navbar navbar-inverse\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;navbar-header\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;navbar-toggle collapsed\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;#navbar\u0026#34; aria-expanded=\u0026#34;false\u0026#34; aria-controls=\u0026#34;navbar\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;Toggle navigation\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;div class=\u0026#34;navbar-brand\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/index\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Game Reviews\u0026lt;/h1\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;navbar\u0026#34; class=\u0026#34;navbar-collapse collapse\u0026#34;\u0026gt; \u0026lt;form class=\u0026#34;navbar-form navbar-right\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;search-filter-text\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Search\u0026#34; class=\u0026#34;form-control\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button id=\u0026#34;search-filter-btn\u0026#34; type=\u0026#34;submit\u0026#34; class=\u0026#34;btn btn-success\u0026#34;\u0026gt;Search\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!--/.navbar-collapse --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;div class=\u0026#34;jumbotron\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {{display_options(\u0026#34;Genres\u0026#34;, \u0026#34;genre\u0026#34;, genres, type=\u0026#34;checkbox\u0026#34;)}} {{display_options(\u0026#34;Platforms\u0026#34;, \u0026#34;platforms\u0026#34;, platforms, type=\u0026#34;checkbox\u0026#34;)}} {{display_options(\u0026#34;Reviewers\u0026#34;, \u0026#34;reviewer\u0026#34;, reviewers, type=\u0026#34;radio\u0026#34;)}} {{display_options(\u0026#34;Posted date\u0026#34;, \u0026#34;posted_date\u0026#34;, dates_intervals, type=\u0026#34;radio\u0026#34;)}} {{display_options(\u0026#34;Sort by\u0026#34;, \u0026#34;order\u0026#34;, {\u0026#34;release_date\u0026#34;:\u0026#34;Posted date\u0026#34;, \u0026#34;score\u0026#34;:\u0026#34;Score\u0026#34;}, type=\u0026#34;radio\u0026#34;)}} \u0026lt;button id=\u0026#34;filter-btn\u0026#34; class=\u0026#34;btn btn-primary\u0026#34; type=\u0026#34;button\u0026#34;\u0026gt;Filter \u0026lt;span class=\u0026#34;glyphicon glyphicon-filter\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;main-content\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; In the html head, not only are we importing css and Javascript files (using url_for() to get its location) as we would do on plain HTML, but we are also defining two Javascript variables that will be available on the global scope: request_params and request_url. The latter is just the URL for “/list” that will be used in the AJAX calls. On the flip side, the former is a Javascript object with the request arguments of the AJAX call. Notice that Flask makes the request object available in the template. An example of request_params would be:\n{ genres: [\u0026#34;Acción\u0026#34;, \u0026#34;RPG\u0026#34;], platforms: \u0026#34;PS4\u0026#34;, posted_date: \u0026#34;Anytime\u0026#34;, reviewer: \u0026#34;any\u0026#34;, sort_by: \u0026#34;score\u0026#34; } In the html body we define the filtering menu and the search bar. Note that we make use of the display_options() macro to avoid code duplication. Also note the empty “main-content” div. Here is where the HTML obtained via AJAX will be located.\nThe view function associated with the second route is more complex as it gets the request arguments, transforms them into a MongoDB query, runs that query and finally renders the HTML that displays re retrieved results. The python code which does that is the following:\ndef extract_eq_filters(): eq_filters = {} for key, val in request.args.items(): if key in [\u0026#34;reviewer\u0026#34;, \u0026#34;platforms\u0026#34;, \u0026#34;genre\u0026#34;]: if key == \u0026#34;reviewer\u0026#34; and val == \u0026#34;any\u0026#34;: continue else: continue eq_filters[key] = val.split(\u0026#39;,\u0026#39;) return eq_filters def extract_range_filters(): range_filters = {} date_interval = request.args.get(\u0026#34;posted_date\u0026#34;, \u0026#34;Anytime\u0026#34;) if date_interval in [\u0026#34;Past week\u0026#34;, \u0026#34;Past month\u0026#34;]: today = datetime.date.now() range_filters[\u0026#34;release_date\u0026#34;] = {\u0026#34;Past week\u0026#34;: (today - datetime.timedelta(days=7), today), \u0026#34;Past month\u0026#34;: (today - datetime.timedelta(days=30), today) }[date_interval] return range_filters @app.route(\u0026#39;/list\u0026#39;) def list_reviews(): if \u0026#34;search\u0026#34; in request.args: reviews = db.search_by_name(coll, request.args.get(\u0026#34;search\u0026#34;)) has_next = False else: eq_filters = extract_eq_filters() range_filters = extract_range_filters() query = db.build_query(eq_filters, range_filters) order = request.args.get(\u0026#34;order\u0026#34;, \u0026#34;release_date\u0026#34;) page_num = int(request.args.get(\u0026#34;page_num\u0026#34;, 0)) reviews, has_next = db.list_reviews(coll, query, page_size=PAGE_SIZE, offset=page_num*PAGE_SIZE, sort_by=order) return render_template(\u0026#39;list.html\u0026#39;, reviews=reviews, has_next=has_next) First, we determine whether the request is a search by name query or a filter query. If it is the former, we retrieve some reviews by using db.search_by_name(). If it is the latter, we extract the equality and range filters, the ordering and the pagination offset from the request object. Next we use db.list_reviews() to retrieve PAGE_SIZE number of reviews. Finally, we render the results in a template.\n{% for review in reviews %} \u0026lt;div class=\u0026#34;review review-{{review.reviewer}}\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;grid-top grid-left\u0026#34; style=\u0026#34;text-align: center;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;score-wrapper\u0026#34;\u0026gt;\u0026lt;span\u0026gt;{{review.score}}\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;grid-top grid-right\u0026#34;\u0026gt; {{ reviewers_logos[review.reviewer] | safe}} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;grid-botton grid-left\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Plataformas\u0026lt;/strong\u0026gt;: {{review.platforms}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Géneros\u0026lt;/strong\u0026gt;: {{review.genre}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Fecha de publicación\u0026lt;/strong\u0026gt;: {{review.release_date}}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;grid-botton grid-right\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;\u0026lt;strong\u0026gt;{{review.title}}\u0026lt;/strong\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;h5\u0026gt;{{review.description}}\u0026lt;/h5\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;clear: both;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; {% endfor %} {% if has_next %} \u0026lt;div style=\u0026#34;text-align: center;\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;next_page\u0026#34; class=\u0026#34;btn btn-primary btn-lg\u0026#34;\u0026gt;Next page\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; {% endif %} Note that if there are more than PAGE_SIZE elements matching the query, we will render a “More Results” button.\nJavascript Earlier in this article, I have stated the reason for using AJAX in our web application: paginate the results coming from the MongoDB collection without having to reload the page each time. Right after the page is loaded an AJAX request will be made to retrieve the first PAGE_SIZE reviews. Thanks to the request_params variable that we created server side, the Javascript code knows the filter parameters to query. Another AJAX request will be made if we click on the “More Results” button. Responses of AJAX requests are inserted in the “main-content” div.\nconst ajax_url = new URL(request_url, document.location); const ajax_params = new URLSearchParams(request_params); ajax_url.search = ajax_params; var page_num = 0; function sendGetRequest(requestUrl, responseHandler,isJsonResponse) { var request = new XMLHttpRequest(); request.onreadystatechange = function() { handleResponse(request, responseHandler, isJsonResponse); }; request.open(\u0026#34;GET\u0026#34;, requestUrl, true); request.send(null); // for POST only  } function handleResponse(request, responseHandler, isJsonResponse) { if ((request.readyState == 4) \u0026amp;\u0026amp; (request.status == 200)) { responseHandler(request.responseText); } } function insert_new_content(main_div, responseText) { var new_div = document.createElement(\u0026#39;div\u0026#39;); new_div.innerHTML = responseText; main_div.appendChild(new_div); var next_page_button = main_div.querySelector(\u0026#34;#next_page\u0026#34;); if (next_page_button){ next_page_button.addEventListener(\u0026#34;click\u0026#34;, next_page, false); } } document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function (event) { sendGetRequest( ajax_url.toString(), function (responseText) { var main_div = document.querySelector(\u0026#34;#main-content\u0026#34;); insert_new_content(main_div, responseText); }); }); function next_page() { page_num += 1; ajax_params.set(\u0026#34;page_num\u0026#34;, page_num); ajax_url.search = ajax_params; console.log(ajax_url.toString()); $ajaxUtils.sendGetRequest( ajax_url.toString(), function (responseText) { var main_div = document.querySelector(\u0026#34;#main-content\u0026#34;); main_div.querySelector(\u0026#34;#next_page\u0026#34;).remove(); insert_new_content(main_div, responseText); }, false); } Conclusion In this post we have created a web application to help us explore the data we collected previously from game reviews websites. We have used Flask to this end as it has all the features we needed while giving us full control over the structure of our project. As for the client side, we made use of AJAX to implement the pagination of the results.\n","date":"2021-02-16T00:00:00Z","image":"https://example.com/p/building-a-website-to-display-the-scraped-data/flask_logo.svg","permalink":"https://example.com/p/building-a-website-to-display-the-scraped-data/","title":"Building a website to display the scraped data"},{"content":"Project Description Videogames are among my favorite hobbies. I usually buy a couple of games every month so that I always have something to play in my spare time. This means I need to spend some time every day reading games reviews and articles in order to to keep myself up to date. Sure this is a good way of taking my mind off work, but lately I am getting a bit tired of it. Could I automate this procress of going trhough several website in search of game reviews?. Of course yes, and that is why in this post we will create together a web scraper to extract reviews from several websites. To this end, we will make use of the web scraping framework Scrapy. The reason for this, is that it comes with a lot of features out of the box which prevent us from reinventing the wheel each time we decide to scrape a new site.\nScrapy Architecture Before diving into web crawling with Scrapy, we will look into its internals, so that we can make a better use of it. Scrapy’s official documentation uses the following diagram to showcase the data flow that takes place under the scenes.\n Scrapy Architecture \nAt first glance, you might think this architecture is a little over-engineered, but trust me, it is much simpler that it seems, plus it allows for a ton of customization. Let\u0026rsquo;s give a brief description of the main components of Scrapy:\nEngine\nThe Engine orchestrates everything that happens inside Scrapy, passing messages between components following the data flow presented above.\nDownloader\nThe Downloader is responsible for downloading webpages. It receives requests from the Engine, sends HTTP requests in order to fetch the webpage and returns the response to the Engine without any further processing.\nScheduler\nThe Scheduler is responsible for timing the web crawling. It receives requests from the Engine and enqueues them following a given policy.\nSpider\nSpiders are responsibles for parsing web pages. After processing a web page the spider can either return additional requests to be enqueued in the Scheduler or items to be processed by the Item Pipeline.\nItem Pipeline\nThe Item pipeline processes the items once they have been extracted by the Spider. Typical uses of the item pipeline are cleaning, validation and storage of the item in a database.\nMiddlewares\nMIddlewares are hooks that sit between the Engine and other components . There are two types of middlewares: Spider middlewares and Downloader Middlewares. In this article we will cover Downloader Middlewares, which process requests coming from the Spider and responses coming from the Downloader.\nSpiders Life Cycle Spiders are basically Python classes that define callback functions to be invoked when responses arrive. In addition, Spiders also define a start_requests() method which defines how to start crawling the site.\nThe life cycle of a Spider is the following:\n The Engine calls the start_requests() method which generates the initial Requests to crawl the site. A Request specifies both the URL and the callback function on the Response. Upon receiving the response, the Engine calls the corresponding method of the Spider. If the callback has been left undefined, the parse() method is called. The result of a callback function can either be an Item or a new Request (which in turn specifies another callback function).  My approach to web scraping In general, each website requires a different scraping strategy to extract the data we are interested in. However, there are navigation patterns that are widely shared across websites. In this project, we are interested in scraping reviews from video game journalism websites so the scraping strategy will be very similar in all scenarios. The general approach when scraping such websites is to start requesting the page where all the games reviews are listed. Then, we extract the URL of each review so that we can navigate to the review page. When processing a review, we could be interested in extracting further information about the game being reviewed (like the genre, release date publisher, etc). To do so, we need the URL of the game info page, which is usually found in the review page. As a recap, the process followed when scraping a game reviews site is the following:\n Extract reviews URLs from the listing page. In the review page, extract relevant information about the review (score, conclusion, positive and negative points, etc) as well as the game info URL. In the game info page, extract information about the game being reviewed.  To take into account this shared navigation pattern, every Spider we create (each spider defines the scraping strategy for a website) will inherit from a Base Spider which defines a template for the scraping process. The code of this Base Spider is the following:\nclass BaseSpider(Spider): def __init__(self, name=None, **kwargs): super(BaseSpider, self).__init__(name=name, **kwargs) def get_reviews_url(self, response): raise NotImplementedError() def request_next_page(self, response): raise NotImplementedError() def request_review(self, url): raise NotImplementedError() def parse(self, response): urls = self.get_reviews_url(response) for url in urls: yield self.request_review(url) yield self.request_next_page(response) BaseSpider is an abstract class which defines methods required for the crawling process. The only method that has a concrete implementation is the parse() method, which is the callback method for requests to the listing page (start_requests() and request_next_page() must produce requests to the listing page). One example of spider which inherits from BaseClass is the following:\nclass IGNSpider(BaseSpider): name = \u0026#34;ign\u0026#34; allowed_domains = [\u0026#34;es.ign.com\u0026#34;] def start_requests(self): return [scrapy_selenium.Request(url=\u0026#34;https://es.ign.com/article/review\u0026#34;, middleware_cls=scrapy_selenium.InfiniteScrollIGN)] def get_reviews_url(self, response): articles = response.xpath(\u0026#34;//section[@class=\u0026#39;broll wrap\u0026#39;]//article\u0026#34;) all_urls = [] all_reviews = [] for article in articles: url = article.xpath(\u0026#34;.//h3/a/@href\u0026#34;).extract_first() if \u0026#34;analisis\u0026#34; in url: review = GameReview() review[\u0026#34;url\u0026#34;] = url img = article.xpath(\u0026#34;.//div[@class=\u0026#39;t\u0026#39;]/a/img\u0026#34;) img_srcset = img.xpath(\u0026#34;@srcset\u0026#34;).extract_first() if img_srcset: img_srcset = [el.strip() for el in img_srcset.split(\u0026#39;,\u0026#39;)] img_url = img_srcset[-1].split(\u0026#39; \u0026#39;)[0] else: img_url = img.xpath(\u0026#34;@src\u0026#34;).extract_first() review[\u0026#34;img_url\u0026#34;] = img_url review[\u0026#34;title\u0026#34;] = article.xpath( \u0026#34;.//div[@class=\u0026#39;m\u0026#39;]/h3/a/text()\u0026#34;).extract_first() review[\u0026#34;description\u0026#34;] = article.xpath( \u0026#34;.//div[@class=\u0026#39;m\u0026#39;]/p/text()\u0026#34;).extract_first() all_urls.append(url) all_reviews.append(review) return all_urls, all_reviews def parse_review_1(self, response): game_review = self.game_reviews[response.request.url] sub_url = urlparse(response.request.url).path.split(\u0026#39;/\u0026#39;)[1] game_review[\u0026#39;platforms\u0026#39;] = [platforms_alias[alias] for alias in platforms_alias if alias in sub_url] review_div = response.xpath(\u0026#34;//div[@class=\u0026#39;review\u0026#39;]\u0026#34;) score = review_div.xpath( \u0026#34;./figure//span[@class=\u0026#39;side-wrapper side-wrapper hexagon-content\u0026#39;]/text()\u0026#34;).extract_first() if not (game_review[\u0026#39;platforms\u0026#39;] and score): self.game_reviews.pop(response.request.url) return game_review[\u0026#39;score\u0026#39;] = float(review_div.xpath( \u0026#34;./figure//span[@class=\u0026#39;side-wrapper side-wrapper hexagon-content\u0026#39;]/text()\u0026#34;).extract_first()) * 10 game_review[\u0026#39;text\u0026#39;] = response.xpath( \u0026#34;//div[@class=\u0026#39;details\u0026#39;]//div[@class=\u0026#39;blurb\u0026#39;]/text()\u0026#34;).extract_first().strip() game_review[\u0026#39;best\u0026#39;] = response.xpath( \u0026#34;//h3[contains(text(), \u0026#39;Pros\u0026#39;)]/following-sibling::ul/li/text()\u0026#34;).extract() game_review[\u0026#39;worst\u0026#39;] = response.xpath( \u0026#34;//h3[contains(text(), \u0026#39;Contras\u0026#39;)]/following-sibling::ul/li/text()\u0026#34;).extract() summary_section = response.xpath( \u0026#34;//section[@class=\u0026#39;object-summary embed\u0026#39;]\u0026#34;) game_review[\u0026#39;game_name\u0026#39;] = summary_section.xpath( \u0026#34;.//h2[@class=\u0026#39;object-title\u0026#39;]/a/text()\u0026#34;).extract_first().strip() follow_up_url = summary_section.xpath( \u0026#34;.//h2[@class=\u0026#39;object-title\u0026#39;]/a/@href\u0026#34;).extract_first() release_date_div = response.xpath( \u0026#34;//div[@class=\u0026#39;article-modified-date\u0026#39;]\u0026#34;) if not release_date_div: release_date_div = response.xpath( \u0026#34;//div[@class=\u0026#39;article-publish-date\u0026#39;]\u0026#34;) release_date = release_date_div.xpath( \u0026#34;./span/text()\u0026#34;).extract_first().strip() release_date = re.search(r\u0026#39; el (.*?) a las \u0026#39;, release_date).group(1) release_date = [el.strip() for el in release_date.split(\u0026#39;de\u0026#39;)] release_date[1] = \u0026#34;{:02d}\u0026#34;.format( month_name.index(release_date[1].lower())) release_date = \u0026#39;-\u0026#39;.join(release_date) game_review[\u0026#39;release_date\u0026#39;] = release_date yield scrapy_selenium.Request(url=follow_up_url, middleware_cls=scrapy_selenium.DynamicContentMiddleware, callback=self.parse_review_2, meta=dict(review_url=response.request.url)) def parse_review_2(self, response): game_review = self.game_reviews[response.meta[\u0026#34;review_url\u0026#34;]] genres = response.xpath( \u0026#34;//dd[@class=\u0026#39;keyword-genre\u0026#39;]/text()\u0026#34;).extract_first() genres = [genre.strip() for genre in genres.split(\u0026#39;/\u0026#39;)] game_review[\u0026#34;genres\u0026#34;] = genres yield game_review def request_review(self, url): return scrapy_selenium.Request(url=url, middleware_cls=scrapy_selenium.DynamicContentMiddleware, callback=self.parse_review_1,) def request_next_page(self, response): return scrapy_selenium.Request.from_response(response) Notice that to fill all the fields in a review Item we need to make two requests, one to the review page and other to the game info page. The methods parse_review_1() and parse_review_2() are the callbacks for each request respectively.\nScraping dynamic websites with Scrapy and Selenium Scrapy is an awesome tool that gives us a complete framework for developing web crawlers, covering aspects from fetching web pages (Downloader), to extract data (Spiders), to validate and store such data (Item Pipelines). However, there is one situation where Scrapy lacks, and this is scraping dynamic websites. This kind of websites make use of Javascript to render content at the client side, which means that some parts of the HTML are generated after the page is loaded or some user interaction has occurred (like pressing a button or scrolling down). This is a problem for vanilla Scrapy because the Downloader does not do any processing on the response, i.e. it does not execute Javascript code. For scraping such websites we need some tool that allows us to programmatically control a web browser. There are plenty of these tools, but for this project we will use Selenium. To combine Scrapy with Selenium we will make use of Downloader Middlewares.\nSimilar to what we did in the previous section, every Middleware will inherit from a Base Class that defines the process to follow when intercepting Requests. The code for the base class is the following:\nclass SeleniumMiddleware: \u0026#34;\u0026#34;\u0026#34;Scrapy middleware handling the requests using selenium\u0026#34;\u0026#34;\u0026#34; def __init__(self, driver_name, driver_arguments): \u0026#34;\u0026#34;\u0026#34;Initialize the selenium webdriver Parameters ---------- driver_name: str The selenium ``WebDriver`` to use driver_arguments: list A list of arguments to initialize the driver \u0026#34;\u0026#34;\u0026#34; driver_class, driver_options_class = {\u0026#39;firefox\u0026#39;: (webdriver.Firefox, webdriver.FirefoxOptions), \u0026#39;chrome\u0026#39;: (webdriver.Chrome, webdriver.ChromeOptions)}[driver_name] driver_options = driver_options_class() for argument in driver_arguments: driver_options.add_argument(argument) self.driver = driver_class( **{f\u0026#39;{driver_name}_options\u0026#39;: driver_options}) self.first_request = True def spider_closed(self): \u0026#34;\u0026#34;\u0026#34;Shutdown the driver when spider is closed\u0026#34;\u0026#34;\u0026#34; self.driver.quit() def _process_request(self, request, spider): raise NotImplementedError() def _page_source(self,): return str.encode(self.driver.page_source) def process_request(self, request, spider): \u0026#34;\u0026#34;\u0026#34;Process a request using the selenium driver if applicable\u0026#34;\u0026#34;\u0026#34; if request.middleware_cls == self.__class__: self._process_request(request, spider) response_html = self._page_source() self.first_request = False return HtmlResponse( self.driver.current_url, body=response_html, encoding=\u0026#39;utf-8\u0026#39;, request=request ) return None In the class constructor, we create an instance of the WebDriver class which allows us to programmatically interact with a browser (in our case, we only consider Firefox and Chrome browsers, but there are other options if needed). When creating a custom Middleware, Scrapy requires us to implement the process_request() method which will be called by the Engine whenever there is a Request ready to be processed. The output of this method can either be a response or None. If it is the former, the response is automatically processed by the corresponding callback. If it is the latter, the Request is passed to the next Middleware. In our scenario, there will be several Middlewares up and running at the same time (one middleware is necessary for each type of interaction), so we need a way to control which one is going to process the request. To achieve that, we store the target Middleware class in the meta attribute of the Request object, so that each Middleware compares the target class with its own and processes the request if there is a match. The _process_request() is an abstract method in which each concrete implementation will define the interaction with the browser via the driver instance. Finally, the _page_source() method returns the HTML generated after the interaction.\nNext I will present two examples of Middlewares: 1) DynamicContentMiddleware, used to fetch a page whose content dynamically generated after receiving the response; and 2) InfiniteScrollMiddleware, used to navigate a listing page in which the content is generated as the user scrolls down.\nclass DynamicContentMiddleware(SeleniumMiddleware): \u0026#34;\u0026#34;\u0026#34;Scrapy middleware handling the requests using selenium\u0026#34;\u0026#34;\u0026#34; def __init__(self, driver_name, driver_arguments, max_delay=10.0, min_delay=7.0): super(DynamicContentMiddleware, self).__init__( driver_name, driver_arguments) self.get_delay = lambda: uniform(max_delay, min_delay) def _process_request(self, request, spider): \u0026#34;\u0026#34;\u0026#34;Process a request using the selenium driver if applicable\u0026#34;\u0026#34;\u0026#34; delay = self.get_delay() print(f\u0026#34;Waiting {delay}seconds ...\u0026#34;) time.sleep(delay) self.driver.get(request.url) class InfiniteScrollIGN(SeleniumMiddleware): def _process_request(self, request, spider): if self.first_request: self.driver.get(request.url) WebDriverWait(self.driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \u0026#34;//section[@class=\u0026#39;broll wrap\u0026#39;]//article//h3/a\u0026#34;))) else: previus_pagenum = int(self.driver.find_elements( By.XPATH, \u0026#34;//section[@class=\u0026#39;broll wrap\u0026#39;]\u0026#34;)[-1].get_attribute(\u0026#34;data-pagenum\u0026#34;)) next_pagenum = previus_pagenum + 1 anchor = self.driver.find_element( By.XPATH, \u0026#34;//div[@id=\u0026#39;infinitescrollanchor\u0026#39;]\u0026#34;) self.driver.execute_script( \u0026#34;arguments[0].scrollIntoView(true)\u0026#34;, anchor) WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \u0026#34;//section[@class=\u0026#39;broll wrap\u0026#39; and @data-pagenum=\u0026#39;{}\u0026#39;]\u0026#34;.format(next_pagenum)))) The DynamicContentMiddleware is pretty simple: it just waits for a random period of time (to minimize the server load while scraping) and then, it loads the requested URL in the driver instance. On the flip side, the InfiniteScrollMiddleware is more complex as it needs to simulate a user scrolling down.\nConclusion In this post we have created a web scraper to help us keep up to date with the latests games releases. To this end we used Scrapy and Selenium, and we came up with a way for them to play together nicely. We make a great effort architecting the system so that we can extend it to scrap new websites without having to reinvent the wheel. If you got interested in this project, the source code is available on my Github.\n","date":"2021-02-16T00:00:00Z","image":"https://example.com/p/scraping-game-reviews-websites-with-scrapy-and-selenium/scrapy_hu70fdb0b361d7eb79037c82a83a9560bd_43195_120x120_fill_box_smart1_2.png","permalink":"https://example.com/p/scraping-game-reviews-websites-with-scrapy-and-selenium/","title":"Scraping Game Reviews Websites with Scrapy and Selenium"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用  思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n 图片  Photo by Florian Klauer on Unsplash   Photo by Luca Bravo on Unsplash \n Photo by Helena Hertz on Unsplash   Photo by Hudai Gayiran on Unsplash \n![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://example.com/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.— Rob Pike1 Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks class DynamicContentMiddleware(SeleniumMiddleware): def __init__(self, driver_name, driver_arguments, max_delay=10.0, min_delay=7.0): super(DynamicContentMiddleware, self).__init__(driver_name, driver_arguments) self.get_delay = lambda: uniform(max_delay, min_delay) def _process_request(self, request, spider): \u0026#34;\u0026#34;\u0026#34;Process a request using the selenium driver if applicable\u0026#34;\u0026#34;\u0026#34; delay = self.get_delay() print(f\u0026#34;Waiting {delay}seconds ...\u0026#34;) time.sleep(delay) self.driver.get(request.url) def _page_source(self,): return str.encode(self.driver.page_source) Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode class DynamicContentMiddleware(SeleniumMiddleware): def __init__(self, driver_name, driver_arguments, max_delay=10.0, min_delay=7.0): super(DynamicContentMiddleware, self).__init__(driver_name, driver_arguments) self.get_delay = lambda: uniform(max_delay, min_delay) def _process_request(self, request, spider): \u0026#34;\u0026#34;\u0026#34;Process a request using the selenium driver if applicable\u0026#34;\u0026#34;\u0026#34; delay = self.get_delay() print(f\u0026#34;Waiting {delay}seconds ...\u0026#34;) time.sleep(delay) self.driver.get(request.url) def _page_source(self,): return str.encode(self.driver.page_source) List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Fruit  Apple Orange Banana   Dairy  Milk Cheese    Other Elements — abbr, sub, sup, kbd, mark GIFis a bitmap image format.\nH2O\nXn+ Yn= ZnPress CTRL+ALT+Deleteto end the session.\nMost salamandersare nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-11T00:00:00Z","image":"https://example.com/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n YouTube Privacy Enhanced Shortcode    Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019  Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }  ","date":"2019-03-10T00:00:00Z","permalink":"https://example.com/p/rich-content/","title":"Rich Content"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","date":"2019-03-05T00:00:00Z","image":"https://example.com/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/emoji-support/","title":"Emoji Support"}]